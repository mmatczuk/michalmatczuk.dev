<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>michalmatczuk.dev/posts/scylladb-manager-24-cloning-a-database-cluster/</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/svg+xml href=/favicon.svg><meta property="og:title" content="ScyllaDB Manager 2.4: Cloning a Database Cluster"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://michalmatczuk.dev/posts/scylladb-manager-24-cloning-a-database-cluster/"><meta property="article:published_time" content="2021-06-30T00:00:00+00:00"><meta name=twitter:title content="ScyllaDB Manager 2.4: Cloning a Database Cluster"><meta name=twitter:description content="This blog post has been first published in ScyllaDB blog.
Cloning a database cluster is probably the most common usage of backup data. This process can be very useful in case of a catastrophic event — say you were running in a single DC and it just burnt down overnight. (For the record, we are always encouraging you to follow our high availability and disaster recovery best practices to avoid such catastrophic failures."><link rel=stylesheet href=https://michalmatczuk.dev/css/terminal.min.css><link rel=stylesheet href=https://michalmatczuk.dev/css/syntax.min.css><link rel=stylesheet href=https://michalmatczuk.dev/css/overwrite.css><link rel=stylesheet href=https://michalmatczuk.dev/css/normalize.min.css><link rel=stylesheet href=https://michalmatczuk.dev/css/fontawesome.min.css><link rel=stylesheet href=https://michalmatczuk.dev/css/brands.min.css><link rel=stylesheet href=https://michalmatczuk.dev/css/solid.min.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-EG92897MK2"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EG92897MK2")</script></head><body class=terminal><div class=container><div class=terminal-nav><header class=terminal-logo><div class="logo terminal-prompt"><a href=https://michalmatczuk.dev class=no-style>michalmatczuk.dev</a>:~#
<a href=https://michalmatczuk.dev/posts>posts</a>/</div></header><nav class=terminal-menu><ul vocab="https://schema.org/" typeof="BreadcrumbList"><li><a href=https://michalmatczuk.dev/posts/ typeof="ListItem">posts/</a></li><li><a href=https://michalmatczuk.dev/talks/ typeof="ListItem">talks/</a></li><li><a href=https://michalmatczuk.dev/projects/ typeof="ListItem">projects/</a></li><li><a href=https://michalmatczuk.dev/about/ typeof="ListItem">about/</a></li></ul></nav></div></div><div class="container animated zoomIn fast"><h1>ScyllaDB Manager 2.4: Cloning a Database Cluster</h1><p><em>This blog post has been first published in <a href=https://www.scylladb.com/author/mmatczuk/>ScyllaDB blog</a>.</em></p><p>Cloning a database cluster is probably the most common usage of backup data.
This process can be very useful in case of a catastrophic event — say you were running in a single DC and it just burnt down overnight.
(For the record, we are always encouraging you to follow our high availability and disaster recovery best practices to avoid such catastrophic failures.
For distributed topologies <a href=https://www.scylladb.com/2021/03/23/kiwi-com-nonstop-operations-with-scylla-even-through-the-ovhcloud-fire/>we have you covered</a> via built-in multi-datacenter replication and ScyllaDB’s fundamental high availability design.) When you have to restore your system from scratch, that process is going to require cloning your existing data from a backup onto your new database cluster.
Beyond disaster recovery, cloning a cluster is very handy in case if you want to migrate a cluster to different hardware, or if you want to create a copy of your production system for analytical or testing purposes.
This blog post describes how to clone a database cluster with ScyllaDB Manager 2.4.</p><p>The latest release of ScyllaDB Manager 2.4 adds a new ScyllaDB Manager Agent <code>download-files</code> command.
It replaces vendor specific tools, like AWS CLI or gcloud CLI for accessing and downloading remote files.
With many features specific to ScyllaDB Manager, it is a “Swiss army knife” data restoration tool.</p><p>The ScyllaDB Manager Agent <code>download-files</code> command allows you to:</p><ul><li>List clusters and nodes in a backup location,
example: <code>scylla-manager-agent download-files -L &lt;backup-location> --list-nodes</code></li><li>List the node’s snapshots with filtering by keyspace / table glob patterns,
example: <code>scylla-manager-agent download-files -L &lt;backup-location> --list-snapshots -K 'my_ks*'</code></li><li>Download backup files to ScyllaDB upload directory,
example: <code>scylla-manager-agent download-files -L &lt;backup-location> -T &lt;snapshot-tag> -d /var/lib/scylla/data/</code></li></ul><p>In addition to that it can:</p><ul><li>Download to table upload directories or keyspace/table directory structure suitable for sstable loader (flag <code>--mode</code>)</li><li>Remove existing sstables prior to download (flag <code>--clear-tables</code>)</li><li>Limit download bandwidth limit (flag <code>--rate-limit</code>)</li><li>Validate disk space and data dir owner prior to download</li><li>Printout execution plan (flag <code>--dry-run</code>)</li><li>Printout manifest JSON (flag <code>--dump-manifest</code>)</li></ul><h2 id=restore-automation>Restore Automation</h2><p>Cloning a cluster from a ScyllaDB Manager backup is automated using the Ansible playbook available in the ScyllaDB Manager repository.
The download-files command works with any backups created with ScyllaDB Manager.
The restore playbook works with backups created with ScyllaDB Manager 2.3 or newer.
It requires token information in the backup file.</p><p>With your backups in a backup location, to clone a cluster you will need to:</p><ul><li>Create a new cluster with the same number of nodes as the cluster you want to clone.
If you do not know the exact number of nodes you can learn it in the process.</li><li>Install ScyllaDB Manager Agent on all the nodes (ScyllaDB Manager server is not mandatory)</li><li>Grant access to the backup location to all the nodes.</li><li>Checkout the playbook locally.</li></ul><p>The playbook requires the following parameters:</p><ul><li><code>backup_location</code> – the location parameter used in ScyllaDB Manager when scheduling a backup of a cluster.</li><li><code>snapshot_tag</code> – the ScyllaDB Manager snapshot tag you want to restore</li><li><code>host_id</code> – mapping from the clone cluster node IP to the source cluster host ID</li></ul><p>The parameter values shall be put into a vars.yaml file, below an example file for a six node cluster.</p><h2 id=example>Example</h2><p>I created a 3 node cluster and filled each node with approx 350GiB of data (RF=2).
Then I ran backup with ScyllaDB Manager and deleted the cluster.
Later I figured out that I want to get the cluster back.
I created a new cluster of 3 nodes, based on i3.xlarge machines.</p><h3 id=step-1-getting-the-playbook>Step 1: Getting the Playbook</h3><p>First thing to do is to clone the ScyllaDB Manager repository from GitHub.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>git clone git@github.com:scylladb/scylla-manager.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> scylla-manager/ansible/restore
</span></span></code></pre></div><p>All restore parameters shall be put to vars.yaml file.
We can copy vars.yaml.example as vars.yaml to get a template.</p><h3 id=step-2-setting-the-playbook-parameters>Step 2: Setting the Playbook Parameters</h3><p>For each node in the freshly created cluster we assign the ID of the node it would clone.
We do that by specifying the <code>host_id</code> mapping in the vars.yaml file.
If the source cluster is running you can use “Host ID” values from “sctool status” or “nodetool status” command output.
Below is a sample “sctool status” output.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ sctool status
</span></span><span class=line><span class=cl>Cluster: <span class=nb>test</span> <span class=o>(</span>77472fd3-a85f-4c8e-bdf5-6c17696ab975<span class=o>)</span>
</span></span><span class=line><span class=cl>Datacenter: dc1
</span></span><span class=line><span class=cl>╭────┬────────────┬──────────┬──────────┬────────────────┬──────────┬──────┬──────────┬────────┬──────────┬──────────────────────────────────────╮
</span></span><span class=line><span class=cl>│    │ Alternator │ CQL      │ REST     │ Address        │ Uptime   │ CPUs │ Memory   │ Scylla │ Agent    │ Host ID                              │
</span></span><span class=line><span class=cl>├────┼────────────┼──────────┼──────────┼────────────────┼──────────┼──────┼──────────┼────────┼──────────┼──────────────────────────────────────┤
</span></span><span class=line><span class=cl>│ UN │ UP <span class=o>(</span>13ms<span class=o>)</span>  │ UP <span class=o>(</span>5ms<span class=o>)</span> │ UP <span class=o>(</span>2ms<span class=o>)</span> │ 192.168.100.11 │ 7h26m34s │ <span class=m>4</span>    │ 31.12GiB │ 4.2.1  │ Snapshot │ 3827c1d1-b300-4093-9839-0cf01f1b9346 │
</span></span><span class=line><span class=cl>│ UN │ UP <span class=o>(</span>9ms<span class=o>)</span>   │ UP <span class=o>(</span>3ms<span class=o>)</span> │ UP <span class=o>(</span>2ms<span class=o>)</span> │ 192.168.100.12 │ 7h26m34s │ <span class=m>4</span>    │ 31.12GiB │ 4.2.1  │ Snapshot │ a4f20c78-0a64-490c-b7f4-f52a62667ff8 │
</span></span><span class=line><span class=cl>│ UN │ UP <span class=o>(</span>14ms<span class=o>)</span>  │ UP <span class=o>(</span>4ms<span class=o>)</span> │ UP <span class=o>(</span>2ms<span class=o>)</span> │ 192.168.100.13 │ 7h26m34s │ <span class=m>4</span>    │ 31.12GiB │ 4.2.1  │ Snapshot │ ffa39462-d3f1-46d9-875c-ef89044e951e │
</span></span><span class=line><span class=cl>╰────┴────────────┴──────────┴──────────┴────────────────┴──────────┴──────┴──────────┴────────┴──────────┴─────────────────────────────────────
</span></span></code></pre></div><p>If the cluster is deleted we can SSH one of the new nodes and list all backed up nodes in the backup location.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ scylla-manager-agent download-files -L s3:manager-test-demo1 --list-nodes
</span></span><span class=line><span class=cl>Cluster: prod <span class=o>(</span>9e9b392b-fdd8-4b36-8980-d937949bf6ce<span class=o>)</span>
</span></span><span class=line><span class=cl>AWS_EU_CENTRAL_1:
</span></span><span class=line><span class=cl> - 18.194.132.192 <span class=o>(</span>3dda17c5-de2a-4bad-80cd-d695c328601a<span class=o>)</span>
</span></span><span class=line><span class=cl> - 18.197.114.43 <span class=o>(</span>b0b60778-f4cd-4d77-857f-5a076cfa938b<span class=o>)</span>
</span></span><span class=line><span class=cl> - 3.66.107.199 <span class=o>(</span>aeca2b65-8e0c-4158-9886-3ffa54451d2f<span class=o>)</span>
</span></span></code></pre></div><p>Based on that information we can rewrite the <code>host_id</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>host_id</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=nt>&#34;35.157.153.136&#34;: </span><span class=l>3dda17c5-de2a-4bad-80cd-d695c328601a</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=nt>&#34;3.68.171.110&#34;: </span><span class=l>b0b60778-f4cd-4d77-857f-5a076cfa938b</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=nt>&#34;18.192.18.219&#34;: </span><span class=l>aeca2b65-8e0c-4158-9886-3ffa54451d2</span><span class=w>
</span></span></span></code></pre></div><p>When we have node IDs we can list the snapshot tags for that node.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ scylla-manager-agent download-files -L s3:manager-test-demo1 --list-snapshots -n 3dda17c5-de2a-4bad-80cd-d695c328601a
</span></span><span class=line><span class=cl>sm_20210624122942UTC
</span></span></code></pre></div><p>We now can set the snapshot_tag parameter as snapshot_tag: <code>sm_20210624122942UTC</code>.</p><p>If you have the source cluster running under ScyllaDB Manager it’s easier to run “sctool backup list” command to get the listing of available snapshots.</p><p>Lastly we specify the backup location as in ScyllaDB Manager.
Below is a full listing of vars.yaml:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=l>$ cat vars.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># backup_location specifies the location parameter used in Scylla Manager</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># when scheduling a backup of a cluster.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>backup_location</span><span class=p>:</span><span class=w> </span><span class=l>s3:manager-test-demo1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># snapshot_tag specifies the Scylla Manager snapshot tag you want to restore.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>snapshot_tag</span><span class=p>:</span><span class=w> </span><span class=l>sm_20210624122942UTC</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># host_id specifies a mapping from the clone cluster node IP to the source</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># cluster host IDs.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>host_id</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=nt>&#34;35.157.153.136&#34;: </span><span class=l>3dda17c5-de2a-4bad-80cd-d695c328601a</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=nt>&#34;3.68.171.110&#34;: </span><span class=l>b0b60778-f4cd-4d77-857f-5a076cfa938b</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=nt>&#34;18.192.18.219&#34;: </span><span class=l>aeca2b65-8e0c-4158-9886-3ffa54451d2f</span><span class=w>
</span></span></span></code></pre></div><p>The IPs nodes in the new cluster must be put to Ansible inventory and saved as hosts:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ cat hosts
</span></span><span class=line><span class=cl>35.157.153.136
</span></span><span class=line><span class=cl>3.68.171.110
</span></span><span class=line><span class=cl>18.192.18.219
</span></span></code></pre></div><h3 id=step-3-check-the-restore-plan>Step 3: Check the Restore Plan</h3><p>Before jumping into restoration right away, it may be useful to see the execution plan for a node first</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ sudo -u scylla scylla-manager-agent download-files -L s3:manager-test-demo1 -n aeca2b65-8e0c-4158-9886-3ffa54451d2f <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-d /var/lib/scylla/data/ --dry-run -T sm_20210624122942UTC
</span></span><span class=line><span class=cl>Cluster:        prod <span class=o>(</span>9e9b392b-fdd8-4b36-8980-d937949bf6ce<span class=o>)</span>
</span></span><span class=line><span class=cl>Datacenter:     AWS_EU_CENTRAL_1
</span></span><span class=line><span class=cl>Node:           3.66.107.199 <span class=o>(</span>aeca2b65-8e0c-4158-9886-3ffa54451d2f<span class=o>)</span>
</span></span><span class=line><span class=cl>Time:           2021-06-24 12:29:42 +0000 UTC
</span></span><span class=line><span class=cl>Size:           324.054G
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Download:
</span></span><span class=line><span class=cl> - system_auth.role_attributes <span class=o>(</span>4.933k<span class=o>)</span> to /var/lib/scylla/data/system_auth/role_attributes-6b8c7359a84333f2a1d85dc6a187436f
</span></span><span class=line><span class=cl> - system_auth.role_members <span class=o>(</span>4.914k<span class=o>)</span> to /var/lib/scylla/data/system_auth/role_members-0ecdaa87f8fb3e6088d174fb36fe5c0d
</span></span><span class=line><span class=cl> - system_auth.role_permissions <span class=o>(</span>42.805k<span class=o>)</span> to /var/lib/scylla/data/system_auth/role_permissions-3afbe79f219431a7add7f5ab90d8ec9c
</span></span><span class=line><span class=cl> - system_auth.roles <span class=o>(</span>47.932k<span class=o>)</span> to /var/lib/scylla/data/system_auth/roles-5bc52802de2535edaeab188eecebb090
</span></span><span class=line><span class=cl> - keyspace1.data_0 <span class=o>(</span>40.431G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_0-5fb8c700d4e011ebbfe5000000000001
</span></span><span class=line><span class=cl> - keyspace1.data_1 <span class=o>(</span>40.448G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_1-5fb89ff0d4e011ebb034000000000002
</span></span><span class=line><span class=cl> - keyspace1.data_2 <span class=o>(</span>40.461G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_2-5fb8c702d4e011ebbfe5000000000001
</span></span><span class=line><span class=cl> - keyspace1.data_3 <span class=o>(</span>40.435G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_3-5fb8c700d4e011eb8930000000000003
</span></span><span class=line><span class=cl> - keyspace1.data_4 <span class=o>(</span>40.954G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_4-5fab5980d4e011eb9c15000000000001
</span></span><span class=line><span class=cl> - keyspace1.data_5 <span class=o>(</span>40.440G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_5-5faba7a0d4e011eba8f8000000000003
</span></span><span class=line><span class=cl> - keyspace1.data_6 <span class=o>(</span>40.428G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_6-619f8860d4e011eb850d000000000001
</span></span><span class=line><span class=cl> - keyspace1.data_7 <span class=o>(</span>40.456G<span class=o>)</span> to /var/lib/scylla/data/keyspace1/data_7-61a072c0d4e011eb8930000000000003
</span></span><span class=line><span class=cl> - system_schema.aggregates <span class=o>(</span>10.533k<span class=o>)</span> to /var/lib/scylla/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895
</span></span><span class=line><span class=cl> - system_schema.columns <span class=o>(</span>108.946k<span class=o>)</span> to /var/lib/scylla/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f
</span></span><span class=line><span class=cl> - system_schema.computed_columns <span class=o>(</span>10.271k<span class=o>)</span> to /var/lib/scylla/data/system_schema/computed_columns-cc7c7069374033c192a4c3de78dbd2c4
</span></span><span class=line><span class=cl> - system_schema.dropped_columns <span class=o>(</span>10.547k<span class=o>)</span> to /var/lib/scylla/data/system_schema/dropped_columns-5e7583b5f3f43af19a39b7e1d6f5f11f
</span></span><span class=line><span class=cl> - system_schema.functions <span class=o>(</span>10.723k<span class=o>)</span> to /var/lib/scylla/data/system_schema/functions-96489b7980be3e14a70166a0b9159450
</span></span><span class=line><span class=cl> - system_schema.indexes <span class=o>(</span>10.600k<span class=o>)</span> to /var/lib/scylla/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f
</span></span><span class=line><span class=cl> - system_schema.keyspaces <span class=o>(</span>98.598k<span class=o>)</span> to /var/lib/scylla/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6
</span></span><span class=line><span class=cl> - system_schema.scylla_tables <span class=o>(</span>100.185k<span class=o>)</span> to /var/lib/scylla/data/system_schema/scylla_tables-5d912ff1f7593665b2c88042ab5103dd
</span></span><span class=line><span class=cl> - system_schema.tables <span class=o>(</span>107.300k<span class=o>)</span> to /var/lib/scylla/data/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09
</span></span><span class=line><span class=cl> - system_schema.triggers <span class=o>(</span>10.203k<span class=o>)</span> to /var/lib/scylla/data/system_schema/triggers-4df70b666b05325195a132b54005fd48
</span></span><span class=line><span class=cl> - system_schema.types <span class=o>(</span>10.322k<span class=o>)</span> to /var/lib/scylla/data/system_schema/types-5a8b1ca866023f77a0459273d308917a
</span></span><span class=line><span class=cl> - system_schema.view_virtual_columns <span class=o>(</span>10.369k<span class=o>)</span> to /var/lib/scylla/data/system_schema/view_virtual_columns-08843b6345dc3be29798a0418295cfaa
</span></span><span class=line><span class=cl> - system_schema.views <span class=o>(</span>13.039k<span class=o>)</span> to /var/lib/scylla/data/system_schema/views-9786ac1cdd583201a7cdad556410c985
</span></span></code></pre></div><p>With <code>--dry-run</code> you may see how other flags like <code>--mode</code> or <code>--clear-tables</code> would affect the restoration process.</p><h3 id=step-4-press-play>Step 4: Press Play</h3><p>It may be handy to configure default user and private key in <code>ansible.cfg</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ cat ~/.ansible.cfg 
</span></span><span class=line><span class=cl><span class=o>[</span>defaults<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>remote_user</span> <span class=o>=</span> support
</span></span><span class=line><span class=cl><span class=nv>private_key_file</span> <span class=o>=</span> /path/to/aws/pem/file
</span></span></code></pre></div><p>When done, “press play” (run the <code>ansible-playbook</code>) and get a coffee.
The restoration took about 15 minutes on a 10Gb network.
The download saturated at approximately 416MB/s.</p><p><img src=cloning-cluster-scylla-monitoring-stack.png alt></p><h3 id=cql-shell-works>CQL Shell Works</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cqlsh&gt; SELECT id FROM keyspace1.data_0 LIMIT 10<span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>id
</span></span><span class=line><span class=cl>--------------------------------------
</span></span><span class=line><span class=cl>b76af606-e6fa-4001-8660-a7c86ea0f545
</span></span><span class=line><span class=cl>71187315-5dd7-4df6-9fc3-324aee6b6456
</span></span><span class=line><span class=cl>a19d1824-6301-41ca-97b9-c85200e8f070
</span></span><span class=line><span class=cl>5d29fd1c-cda3-4efe-b6d7-96061c2a5257
</span></span><span class=line><span class=cl>e4c1411b-7de5-45d6-b375-d02332f0394c
</span></span><span class=line><span class=cl>4b3ca60a-b1e3-410d-ad7a-27c9ab22734a
</span></span><span class=line><span class=cl>a302b84c-3da3-468c-a628-28000dd5cc7f
</span></span><span class=line><span class=cl>debb2d34-f107-41c1-b3e1-49feed15982b
</span></span><span class=line><span class=cl>ba660b34-d2b1-4e1e-859a-f9bc66617ca6
</span></span><span class=line><span class=cl>93a39da5-c23b-4590-952c-1e86a09d60a9
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>(</span><span class=m>10</span> rows<span class=o>)</span>
</span></span></code></pre></div><p>After completing the restore, it’s recommended to run a repair with ScyllaDB Manager.</p><h3 id=next-steps>Next Steps</h3><p>If you want to try this out yourself, you can get a hold of ScyllaDB Manager either as a ScyllaDB Open Source user (for up to five nodes), or as a ScyllaDB Enterprise customer (for any sized cluster).
You can get started by heading to our Download Center, and then checking out <a href=https://github.com/scylladb/scylla-manager/tree/master/ansible/restore>the Ansible Playbook</a> in our ScyllaDB Manager Github repository.</p><div class=footer><a href=https://github.com/mmatczuk><i class="fa-brands fa-github"></i></a>
<a href=https://twitter.com/michalmatczuk><i class="fa-brands fa-twitter"></i></a>
<a href=https://www.linkedin.com/in/mmatczuk><i class="fa-brands fa-linkedin"></i></a></div></div></body></html>